# CSCI_4100_OS_10_26_2018.md

## Assignment talk

remember to start threads then call join on all of them

```c
std::thread t(...)
philThreads[i] = new Thread(...)
philThreads[i]-> join();

new std::threads(startPhil(...)) // this can't happen b/c startPhilc is void
```

Ok, back to class stuff

## Multiple processors

---

### Option 1: use a centralized MFQ (Multi-level Feedback Queue) with a lock to gaurd it

* Contention for the lock may become a bottleneck
* The state of MFQ will need to be fetched from the cache of the last processor to access it adding overhead
  * when you actually load something from memory. Those changes usually happen on a cache.
  * any time a processor makes modifications, it changes the cache
  * in a single processor system, it doesn't matter so much how often that gets flushed
* Threads may be scheduled on processors that do not have their data cached, impacting performance

### Option 2: each processor has its own MFQ

* __Affinity Scheduling__ - threads are typically scheduled on the processor they last ran on
  * if the system has not to much work to do, then this works pretty well
* If MFQ's become too imbalanced, threads can be moved to different MFQ

## Parallel applications

* Option 1: __oblivious scheduling__
  * __Bulk scheduling parallelism__ such that blocks until all threads complete a stage in a computation
    * may be stalled if one thread is `preempted`
  * __Producer-consumer__ model can be completely stalled if one of the thread in the chain is preempted
    * sets up a set of dependencies
    * a chain is only as strong as its weakest link
  * Preempting threads in a the __critical path__ of a parallel application can slow down progress.
  * If the holder of a lock is preempted, threads waiting to acquire the lock will not be able to make progress
    * what can we do about this???
    * well.. let's be less "oblivious"

* Option 2: __gang scheduling__

  * all threads of an application must be run together or not at all
    * all for one, and one for all!!!
    * Musketeer scheduling
  * most systems allow you to dedicate a set of processors to an particular application
  * Some apps that are very parallel an benefit from multi-processor techniques
  * Some apps may see diminishing returns when adding additional processors
  * The number of processors available may change over time

* Option 3: __scheduler activations__

  * An application is informed by the OS each time a processor is made assigned to it
  * If a thread blocks, the processor it is runnning on gets reassigned to another application
  * This means we now have 2 diff schedulers
  * The OS schedules applications on the processors, applications schedule threads on scheduler activations
  * __Multiprocessor scheduling policy__ 
    * How many processors should be assigned to each application?

## Energy-aware scheduling

* Low-power processors are usually slower and simpler
* some systems have a high power processor for performance and a low power processor to conserve energy
* On multi-core systems lightly used processors are disabled to conserve power
* Same with I/O devices not in use, can be powered off...
* Important to consider the impact on response time.
