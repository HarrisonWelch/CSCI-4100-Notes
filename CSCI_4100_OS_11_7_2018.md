# CSCI_4100_OS_11_7_2018.md

A __cache__ is a copy of a computation or data that can be accessed more quickly than the original

We have already looked at ex of hardware cache

## Hardware Caches

* TLBs
* Virtually addressed caches 
* Physically addressed caches

## Other uses of caches

* Translations of domain names to IP addressed - cached by a web client
* Content of web pages are cached by web client
* Web search engines cache copies of web pages
* Email clients store copies emails on client computers
* Virtual memory systems use main memory as a cache for process images stored
* File system use memory as a cache for frequently used files and directories
* Process cache results of executing computation
  * speculative execution where when a processor hits a branch (decision on execution)

## Design challenges

* Location the cached copy MUST BE EFFICIENT
  * has to be faster than main memory (original)
  
"Caches are dynamic"

"Caches are finite in size"

"if you bring something in you must kick something out"

* Replacement Policy
  * which current entry should be replaced be a new entry
* Cache coherence
  * dealing with invalid cache entries

A memory cache stores pairs of addresses and the values stored at those addresses

* A __cache hit__ occurs when a cache contains an entry for a requested address
* A __cache miss__ occurs when a cache does not contain an entry for a requested address
* Cache efficiency: cache hit must be significantly faster than a cache miss
* A program with __temporal locality__ tends to access recently accessed data.
* A program with __spatial locality__ not only tends to reference recent data and data it's close to in recent memory

A read operation checks the cache to see if it contains the required data

`latency_{read} = P(hit) * Latency_{hit} + P(miss) * Latency_{miss}`

* temporal locality - cache stores recently accessed data -> higher hit rate
* spatial locality - store blocks of data (a bunch of things at a time); or __"pre-fetch"__ data -> higher hit rate

## Most systems buffer write operations

* If there is room for the buffer, the computation can continue while data is transferred to the cache are immediatly sent to memory
* __write-through__ cache - all updates are immediately sent to memory
* __write-back__ cache - updates are sent to memory when the cache evicts a block that has been written to.

## Tradeoff between memory that is small, fast, and expensive and memory that is large, slow, and cheap.

"idealy we want __LARGE, FAST, and CHEAP__"

## On-chip cache

* First-level VA cache - 64 KB, 1 nanoseconds

| Type | Size  | Time  |
| ---- | ------ | ----- |
| 1st-level cache  | 64  KB | 1  ns |
| 2nd-level cache  | 256 KB | 4  ns |
| 3rd-level cache  | 2   MB | 12 ns |

* First and second level TLBs are similar to 1st and 2nd level of caches

## Main memory

* local memory - 16 GB - BUT this is a lot slower at 100 ms

| Type | Size  | Time  |
| ---- | ------ | ----- |
| Local Memory  | 16  GB | 100  ns |
| LAN using cooperative caching  | ??? TB | 100  us |

## Non-volatile storage devices

* NV memory (flash, SSD, HD) - 100s GB


| Type | Size  | Time  |
| ---- | ------ | ----- |
| NV memory  | 100 GB | 100  us |
| Local Magnetic disk  | 1 TB | 10  ms |
| LAN disk | 100 PB (above TB) | 10 ms |
| remote disk | 1 XB (above PB) | 200 ms |

# All together

| Type | Size  | Time  |
| ---- | ------ | ----- |
| 1st-level cache  | 64  KB | 1  ns |
| 2nd-level cache  | 256 KB | 4  ns |
| 3rd-level cache  | 2   MB | 12 ns |
| Local Memory  | 16  GB | 100  ns |
| LAN using cooperative caching  | ??? TB | 100  us |
| NV memory  | 100 GB | 100  us |
| Local Magnetic disk  | 1 TB | 10  ms |
| LAN disk | 100 PB (above TB) | 10 ms |
| remote disk | 1 XB (above PB) | 200 ms |
