# CSCI_4100_OS_11_9_2018.md

Assignment not out yet

## Back to the lesson planned

Effectiveness of a cache is determined by both its size and by program behavior

Most programs have a critical mass of data called a __working set__ that is accessed most frequently

* If the working set can fit in the cache, the hit rate will
* Hit rates can be impacted by change in program behavior, and process switches
* Size of working set can impact performance as well as which level of the heirarchy fits it best

Larger applications like web proxy caches encounter significantly more data with no discernable working set.

* Frequency of visits to work pages can be modeled using a Zipf distribution
* For alpha between 1 and 2, the frequency of visits to the leth most popular web page is 1/k*alpha
* As the size of a cache increases exponentially the hit rate only grows linearly

|rank|relative freq.|
|--|--|
1|f
2| (1/2) f
3| (1/3) f
4| (1/4) f
5| (1/5) f
6| (1/6) f

## Types of caches

Software caches can use the same techniques used for addr translation/lookup

Hardware caches must use techniques that don't require a whole lot of overhead, any additional overhead causes a significant amount of time. Want very low latency

A __fully associative__ cache allows an address to be stored anywhere in the table

* System must check an address against all (essentially in parallel) entries simultaneously
* Very fast, but very expensive
* Not pratical for large caches

A __direct mapped__ cache only allows an address to be stored at one location in the table

* Lookup is performed by hashing the address to its entry
* Efficient regardless of the size of the cache
  * but less flexible
* If there is collision (two addresses mapping to the same location)

A __set associative__ cache combines the previous approaches

* A k-associative cache allows an address to be stored in any of a bucket of k different entries in the table
* Direct-mapped approach used to find bucket
* fully-assoc. to find stuff in the bucket
* Almost universally used, almost all hardware caches and TLB use this method -> set-assoc. approach
* OSs can use page coloring. It's page frames map to different buckets

~~"We've talked about looking stuff up, how about eviction?"

## Replacement policy

### Random - pick an arbitrary block to replace

* Low overhead -> good for a first-level cache
* On average, Random won't make the best choice or worst choice
* Unpredictable, difficult for an app to optimize for it.

### First-in-first-out - evict block that's been in here the longest

* cache is treated like a FIFO queue
* simple to implement
* can perform poorly in certain situations
* if an algorithm cycles through the elements of an array that itself is too large to fit in the cache -> we get repeated cache misses

```

Cache addr.s | memory blocks in array
             | A | B | C | D | E | A | B | C | D | E | A | B | C | D | E |
---------------------------
0            | A               E (replace A)
1            |    B               A (replace B)
2            |       C                ...
3            |          D
```

THRASHING!!

The optimal cache policy (MIN) replaces whatever block is farthest in the future

```

Cache addr.s | memory blocks in array
             | A | B | C | D | E | A | B | C | D | E | A | B | C | D | E |
---------------------------
0            | A               E   +                   +
1            |     B                   +                   +   C
2            |         C                   +   D                   +
3            |             D   E                   +                   +
```

* Like SJF, MIN requires knowledge of the future
* __Prefetching__ multiple block at a time can make this even more efficient

### Least Recently used (LRU) - replaces the block referenced in the past

* a software cache - moving a block to the front of a list when referenced, evict block at end
* LRU can be optimal if a program has temporal locality
* If least recently used block is the most to be refed the LRU behaves like FIFO

### Least Frequently used (LFU) - evicts the block that is used the least frequently

* LRU treats frequently & infrequently used memory block
* LRU is better for a working set, LFU is a better at predicting which blocks will be used in the future.